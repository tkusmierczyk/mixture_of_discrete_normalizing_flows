{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YvryyD5UiqVT"
   },
   "source": [
    "# Categorical VAE with Gumbel-Softmax\n",
    "\n",
    "Partial implementation of the paper [Categorical Reparameterization with Gumbel-Softmax](https://arxiv.org/abs/1611.01144) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation follows tightly the code from https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5PUK6xQMiqVi"
   },
   "source": [
    "## Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cf1nacIdiqVo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "0w2iD0BMiqV3",
    "outputId": "a04ed99f-73b6-4646-f83b-44d86cc30ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.2.0', '0.9.0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__, tfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../mdnf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aux\n",
    "import time\n",
    "import copy\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be run as a script with args in format KEY=VAL,KEY=[STRVAL],...\n",
    "args = aux.parse_script_args() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = args.get(\"SEED\", 0)\n",
    "OUT = args.get(\"OUT\", \"VAEConcrete.csv\")\n",
    "\n",
    "ST = bool(args.get(\"ST\", 0))\n",
    "LOSS = args.get(\"LOSS\", 3)\n",
    "N = args.get(\"N\", 10) # how many latent variables\n",
    "K = args.get(\"K\", 20) # how many categories each\n",
    "\n",
    "BASE_TEMP=args.get(\"BASE_TEMP\", 1.0) # initial temperature\n",
    "ANNEAL_RATE=args.get(\"ANNEAL_RATE\", 0.00003)\n",
    "MIN_TEMP=args.get(\"MIN_TEMP\", 0.5)\n",
    "PRIORS_TEMP=args.get(\"PRIORS_TEMP\", 1.0) # ignored with the Jang's loss\n",
    "\n",
    "OPTIMIZER = args.get(\"OPTIMIZER\", \"ADAM\")\n",
    "LR = args.get(\"LR\", 0.001)\n",
    "BATCH_SIZE=args.get(\"BATCH_SIZE\", 256) # how many samples in minibatch\n",
    "NUM_ITERS=args.get(\"NUM_ITERS\", 100) # how many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CFG=['VAE_CONCRETE_JANG', 0, 'VAEConcrete.csv', False, 10, 20, 256, 4, 1.0, 1.0, 'ADAM', 0.001, 3e-05, 0.5]\n"
     ]
    }
   ],
   "source": [
    "assert LOSS==3, \"This implementation supports only LOSS==3\"\n",
    "ALG_NAME = {0: \"VAE_CONCRETE\", \n",
    "            1: \"VAE_CONCRETE_MADDISON21\", \n",
    "            2: \"VAE_CONCRETE_MADDISON22_JANG2\", \n",
    "            3: \"VAE_CONCRETE_JANG\",\n",
    "            4: \"VAE_CONCRETE_CATPRIORS\"}[LOSS]\n",
    "CFG = [ALG_NAME, SEED, OUT, ST, N, K, BATCH_SIZE, NUM_ITERS, \n",
    "       BASE_TEMP, PRIORS_TEMP, OPTIMIZER, LR, ANNEAL_RATE, MIN_TEMP]\n",
    "print(\"CFG=%s\" % CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K18in8SZiqWQ"
   },
   "source": [
    "##  Gumbel-Softmax & Straight-Through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5HV9VIGiqWT"
   },
   "outputs": [],
   "source": [
    "@tf.function       \n",
    "def straight_through_sample(y, hard=True):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    y: [..., n_class] one-hot sample from the Gumbel-Softmax distribution. \n",
    "    hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
    "  Returns:\n",
    "    If hard=True, then the returned sample y will be one-hot, otherwise it will\n",
    "    be a probabilitiy distribution that sums to 1 across classes\n",
    "  \"\"\"\n",
    "  if hard:\n",
    "    K = y.shape[-1]\n",
    "    y_hard = tf.cast(tf.one_hot(tf.argmax(y,-1), K), y.dtype)\n",
    "    y = tf.stop_gradient(y_hard - y) + y  \n",
    "  return y\n",
    "\n",
    "\n",
    "@tf.function        \n",
    "def sample_gumbel(shape, eps=1e-20): \n",
    "  \"\"\"Sample from Gumbel(0, 1)\"\"\"\n",
    "  U = tf.random.uniform(shape,minval=0,maxval=1)\n",
    "  return -tf.math.log(-tf.math.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "@tf.function   \n",
    "def gumbel_softmax_sample(logits, temperature): \n",
    "  \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "  y = logits + sample_gumbel(tf.shape(logits))\n",
    "  return tf.nn.softmax( y / temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zXgNCssWiqWd"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U8bqeCYViqWg"
   },
   "outputs": [],
   "source": [
    "class CategoricalVAE(tf.Module):\n",
    "\n",
    "    def __init__(self, N, K, hard=False, name=None):\n",
    "        \"\"\"\n",
    "        \n",
    "            Args:\n",
    "                N: number of variables\n",
    "                K: number of categories for each variable\n",
    "                hard: set hard=True for ST Gumbel-Softmax \n",
    "        \"\"\"\n",
    "        super(CategoricalVAE, self).__init__(name=name)\n",
    "\n",
    "        self.N = N # number of categorical distributions\n",
    "        self.K = K # number of classes\n",
    "        self.hard = hard # set hard=True for ST Gumbel-Softmax \n",
    "\n",
    "        self.calc_logits_y = tf.keras.Sequential([ # encoder\n",
    "                                             tf.keras.layers.Flatten(name=\"encoder0\"),\n",
    "                                             tf.keras.layers.Dense(512, activation=\"relu\", \n",
    "                                                                   input_shape=(None, 784), name=\"encoder1\"),\n",
    "                                             tf.keras.layers.Dense(256, activation=\"relu\", name=\"encoder2\"),\n",
    "                                             tf.keras.layers.Dense(K*N, activation=None, name=\"encoder3\"),\n",
    "                                             tf.keras.layers.Reshape( [N,K] , name=\"encoder4\")\n",
    "                                            ], name=\"encoder\")\n",
    "\n",
    "        self.calc_logits_x = tf.keras.Sequential( # decoder\n",
    "                                            [tf.keras.layers.Flatten(name=\"decoder0\"),\n",
    "                                             tf.keras.layers.Dense(256, activation=\"relu\", name=\"decoder1\"),\n",
    "                                             tf.keras.layers.Dense(512, activation=\"relu\", name=\"decoder2\"),\n",
    "                                             tf.keras.layers.Dense(784, activation=None, name=\"decoder3\"),\n",
    "                                            ], name=\"decoder\") \n",
    "    \n",
    "    def __call__(self, x, temperature=5.0):\n",
    "        tau = tf.constant(temperature, name=\"temperature\", dtype=tf.float32)                        \n",
    "        \n",
    "        # variational posterior q(y|x), i.e. the encoder \n",
    "        # unnormalized logits for N separate K-categorical distributions \n",
    "        # (shape=(batch_size,N,K))\n",
    "        logits_y = self.calc_logits_y(x)\n",
    "        q_y = tf.nn.softmax(logits_y)                        \n",
    "        y = gumbel_softmax_sample(logits_y, tau)  \n",
    "        y = straight_through_sample(y, self.hard) # set hard=True for ST Gumbel-Softmax\n",
    "\n",
    "        # generative model p(x|y)\n",
    "        logits_x = self.calc_logits_x(y)    \n",
    "        p_x = tfpd.Bernoulli(logits=logits_x)\n",
    "        \n",
    "        return p_x, q_y, y, logits_y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2AC1b8njiqWy"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0K0W_U7iqW9"
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "frmt = lambda images: tf.cast(tf.reshape(images,(-1, 784)), tf.float32)/255.0\n",
    "x_train, x_test = frmt(x_train), frmt(x_test)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l8qA4dnZiqXP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fb060446c40> lr=0.001\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZERS = {\"RMS\": tf.keras.optimizers.RMSprop,\n",
    "              \"ADAM\": tf.keras.optimizers.Adam}\n",
    "if OPTIMIZER not in OPTIMIZERS: raise ValueError(\"Unknown optimizer!\")\n",
    "optimizer_class = OPTIMIZERS[OPTIMIZER]\n",
    "optimizer = optimizer_class(learning_rate=LR)\n",
    "\n",
    "print(\"optimizer=%s lr=%s\" % (optimizer, LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_Jang(x, p_x, q_y, y):\n",
    "    \"\"\" Matching loss from\n",
    "        https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb\n",
    "    \"\"\"\n",
    "    batch_size, N, K = q_y.shape\n",
    "\n",
    "    log_q_y = tf.math.log(q_y+1e-20)       \n",
    "    \n",
    "    KL = tf.reshape(q_y*(log_q_y-tf.math.log(1.0/K)),[-1,N,K])\n",
    "    KL = tf.reduce_sum(KL, [1,2])\n",
    "\n",
    "    elbo = tf.reduce_sum(p_x.log_prob(x), 1) - KL        \n",
    "    return tf.reduce_mean(-elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_loss(x, p_x, q_y, logits_y, nsamples=1000, *args):\n",
    "    batch_size, N, K = q_y.shape\n",
    "    \n",
    "    samples = tf.stack([gumbel_softmax_sample(logits_y, 1.0) for _ in range(nsamples)])\n",
    "    py = tf.reduce_mean(straight_through_sample(samples), 0)\n",
    "    H_q = -tf.reduce_sum(py*np.log(py+1e-16), [-1,-2]) # sum over categories and then over independent variables\n",
    "\n",
    "    Eq_logp = np.sum(np.mean(np.ones((batch_size,N,K))*np.log(1/K), -1), -1)\n",
    "\n",
    "    KL = -Eq_logp -H_q\n",
    "    lik = tf.reduce_sum(p_x.log_prob(x), -1)\n",
    "    return float(-tf.reduce_mean(lik)+tf.reduce_mean(KL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tloGJnJpf6aa"
   },
   "outputs": [],
   "source": [
    "vae = CategoricalVAE(N=N, K=K, hard=ST)\n",
    "results = []\n",
    "best_loss, best_vae = float(\"inf\"), None\n",
    "start = time.time()\n",
    "i = 0\n",
    "np_temp=BASE_TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = <function loss_Jang at 0x7fb081d0b3a0>\n"
     ]
    }
   ],
   "source": [
    "loss = {\n",
    "        3: loss_Jang\n",
    "}[LOSS]\n",
    "print(\"loss = %s\" % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-bi8Uc8iiqXV",
    "outputId": "29b7c1df-8a26-45e7-fe1b-8d89084c54dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2s] epoch=0/iteration=1 loss=543.39 ELBO=-543.48\n",
      "[1.8s] epoch=0/iteration=2 loss=537.70 ELBO=-537.79\n",
      "[2.4s] epoch=0/iteration=3 loss=531.24 ELBO=-531.33\n",
      "[3.1s] epoch=0/iteration=4 loss=522.39 ELBO=-522.48\n",
      "[3.7s] epoch=0/iteration=5 loss=510.15 ELBO=-510.24\n",
      "[4.3s] epoch=0/iteration=6 loss=494.42 ELBO=-494.51\n",
      "[5.0s] epoch=0/iteration=7 loss=470.15 ELBO=-470.25\n",
      "[5.6s] epoch=0/iteration=8 loss=443.06 ELBO=-443.16\n",
      "[6.3s] epoch=0/iteration=9 loss=409.12 ELBO=-409.21\n",
      "[62.1s] epoch=0/iteration=100 loss=196.48 ELBO=-196.58\n",
      "[124.9s] epoch=0/iteration=200 loss=188.83 ELBO=-188.93\n",
      "[151.6s] 0. l=213.82 (best=213.82) ELBO=-213.91 t=1.00\n",
      "Saving to VAEConcrete.csv\n",
      "[201.5s] epoch=1/iteration=300 loss=166.84 ELBO=-166.94\n",
      "[281.0s] epoch=1/iteration=400 loss=159.60 ELBO=-159.69\n",
      "[336.0s] 1. l=169.31 (best=169.31) ELBO=-169.40 t=1.00\n",
      "Saving to VAEConcrete.csv\n",
      "[360.1s] epoch=2/iteration=500 loss=146.04 ELBO=-146.14\n",
      "[441.7s] epoch=2/iteration=600 loss=145.43 ELBO=-145.52\n",
      "[509.6s] epoch=2/iteration=700 loss=137.57 ELBO=-137.65\n",
      "[512.6s] 2. l=145.49 (best=145.49) ELBO=-145.58 t=1.00\n",
      "Saving to VAEConcrete.csv\n",
      "[572.8s] epoch=3/iteration=800 loss=129.58 ELBO=-129.66\n",
      "[636.0s] epoch=3/iteration=900 loss=134.58 ELBO=-134.66\n",
      "[661.2s] 3. l=134.42 (best=134.42) ELBO=-134.51 t=1.00\n",
      "Saving to VAEConcrete.csv\n"
     ]
    }
   ],
   "source": [
    "for e in range(NUM_ITERS):\n",
    "\n",
    "    losses, true_losses = [], []\n",
    "    for np_x, labels in train_ds:\n",
    "        \n",
    "        with tf.GradientTape() as tape:        \n",
    "            p_x, q_y, y, logits_y = vae(np_x, temperature=np_temp)\n",
    "            l = loss(np_x, p_x, q_y, y)\n",
    "        losses.append( float(l) ); \n",
    "        tl = true_loss(np_x, p_x, q_y, logits_y)\n",
    "        true_losses.append( float(tl) )\n",
    "        g = tape.gradient(l, vae.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, vae.trainable_variables))\n",
    "        \n",
    "        if i % 1000 == 1: # following Jang's code\n",
    "            np_temp = np.maximum(BASE_TEMP*np.exp(-ANNEAL_RATE*i), MIN_TEMP)\n",
    "            \n",
    "        i += 1\n",
    "        if i%100==0 or i<10: \n",
    "            print(\"[%.1fs] epoch=%i/iteration=%i loss=%.2f ELBO=%.2f\" % \\\n",
    "                  (time.time()-start, e, i, l, -tl))    \n",
    "    ########################################################################\n",
    "    \n",
    "    if np.mean(losses)<best_loss:\n",
    "        best_loss = np.mean(losses)\n",
    "        try: \n",
    "            best_vae = copy.deepcopy(vae)\n",
    "        except Exception as exc:\n",
    "            if best_vae is None:\n",
    "                    print(\"[ERROR] Failed to copy VAE object as best_vae: %s\" % exc)\n",
    "            best_vae = vae                \n",
    "            \n",
    "    print (\"[%.1fs] %d. l=%.2f (best=%.2f) ELBO=%.2f t=%.2f\" % (time.time()-start, \n",
    "            e, np.mean(losses), best_loss, -np.mean(true_losses), np_temp))                        \n",
    "    results.append(CFG+[time.time()-start, \n",
    "            e, np.mean(losses), best_loss, np.mean(true_losses), np_temp])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    print(\"Saving to %s\" % OUT)\n",
    "    pd.DataFrame(results).to_csv(OUT, header=False, index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CategoricalVAE_TF2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
